---
title: 3 5. Modeling tuning & Operation
categories:
  - machine_learning
toc: true
---

# **ğŸš© Chapter 05**  

---
> **ëª©ì°¨(Context)**

* 01.Hyper parameter tunning
* 02.Model explanation
* 03.Modeling operation

## **01. Hyper parameter tunning**
---
> **Parameter vs Hyper-parameter** 

* Parameter : ëª¨ë¸ ë‚´ë¶€ì—ì„œ ì •í•´ì§€ëŠ” ë³€ìˆ˜
	 ex) íšŒê·€ì‹ì—ì„œ íšŒê·€ê³„ìˆ˜
* Hyper-parameter : ìµœì ì˜ paramterë¥¼ ë„ì¶œí•˜ê¸° ìœ„í•´ ìš°ë¦¬ê°€ ì§ì ‘ ì„¸íŒ…í•˜ëŠ” ê°’
	 ex) knnì—ì„œ kì˜ ê°’, Random Forestì—ì„œ max_depthì˜ ê°’ ë“±

> **ëŒ€í‘œì  ë°©ë²•ë¡ ** 

* Grid-Search : í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ë³€ê²½í•˜ë©° ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì•„ê°€ëŠ” ê¸°ë²•, ëª¨ë“  ê²½ìš°ì˜ ìˆ˜ë¥¼ íƒìƒ‰
* Random Search : ì •í•´ì§„ ë²”ìœ„ ë‚´ì—ì„œ Randomí•˜ê²Œ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ íƒìƒ‰
* **BayesianOptimization** : ì…ë ¥ê°’(x)ë¥¼ Inputìœ¼ë¡œ í•˜ëŠ” ëª©ì  í•¨ìˆ˜ f(x)ë¥¼ ì°¾ëŠ”ê²ƒ, x(hyper-parameter) / f(x)(precision, recall, AUC ë“±)
- ìˆœì°¨ì ìœ¼ë¡œ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•´ ê°€ë©´ì„œ í‰ê°€ë¥¼ í†µí•´ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì„ íƒìƒ‰ 

```python
# â–¶ BayesianOptimization ì„¤ì¹˜
!pip install bayesian-optimization
```

> ### **â”” Random Forest Opt** 

```python
# â–¶ BayesianOptimization
import numpy as np
from bayes_opt import BayesianOptimization
from sklearn.model_selection import cross_val_score


def model_evaluate(n_estimators, maxDepth):
    clf = RandomForestClassifier(
        n_estimators= int(n_estimators),
        max_depth= int(maxDepth))
    scores = cross_val_score(clf, x_train, y_train, cv=5, scoring='roc_auc')
    return np.mean(scores)
    
    
def bayesOpt(x_train, y_train):
    clfBO = BayesianOptimization(model_evaluate, {'n_estimators':  (100, 200),
                                                  'maxDepth': (2, 4)
                                                 })
    clfBO.maximize(init_points=5, n_iter=10)
    print(clfBO.res)

bayesOpt(x_train, y_train)
```
- n_estimators, maxdepth 2ê°œì— ëŒ€í•´ tuning ì§„í–‰
- cross validation ì ìˆ˜ ê³„ì‚°
- cv=5: 5ë²ˆ ëŒë¦¬ê³  í‰ê·  ë‚´ë¼
- init_points: ì‹œì‘ í¬ì¸íŠ¸
- n_iter: ì‹œì‘ í¬ì¸íŠ¸ ì´í›„ë¡œ ëª‡ ë²ˆ ë” ëŒê±´ì§€
![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240116145800.png?raw=true)
- ë¶„í™ ê°’ìœ¼ë¡œ tuning í•´ì£¼ë©´ ë¨: max depth:3, n_est: 196

```python
# â–¶ RandomForest
from sklearn.ensemble import RandomForestClassifier
import sklearn.metrics as metrics
from sklearn.metrics import roc_auc_score

rfc = RandomForestClassifier(n_estimators=196, max_depth = 3)
rfc.fit(x_train, y_train)

# â–¶ ì˜ˆì¸¡
y_pred_train = rfc.predict(x_train)
y_pred_test = rfc.predict(x_test)

y_pred_train_proba = rfc.predict_proba(x_train)[:, 1] 
y_pred_test_proba = rfc.predict_proba(x_test)[:, 1] 


rfc_re = pd.DataFrame({ 'model' : ['RFC(BO)'],
                      'f1_train' :  metrics.f1_score(y_train,y_pred_train),
                      'f1_test' : metrics.f1_score(y_test,y_pred_test),
                      'AUC_train' : roc_auc_score(y_train, y_pred_train_proba),
                      'AUC_test' : roc_auc_score(y_test, y_pred_test_proba),}
                    )

df_comparison = df_comparison.append(rfc_re)
df_comparison.reset_index(drop=True, inplace = True)
```
- ì„±ëŠ¥ë„ ì˜¬ë¼ê°€ê³  overfittingë„ ì¡í˜

```python
df_comparison
```

> ### **â”” LightGBM Opt** 

```python
def lgb_evaluate(n_estimators, maxDepth):
    clf = LGBMClassifier(
        objective = 'binary',
        metric= 'auc',
        learning_rate=0.01,
        n_estimators= int(n_estimators),
        max_depth= int(maxDepth))
    scores = cross_val_score(clf, x_train, y_train, cv=5, scoring='roc_auc')
    return np.mean(scores)
    
def bayesOpt(train_x, train_y):
    lgbBO = BayesianOptimization(lgb_evaluate, {                                               
                                                'n_estimators': (100, 200),
                                                'maxDepth': (2, 4)  
                                               })
    lgbBO.maximize(init_points=5, n_iter=10)
    print(lgbBO.res)

bayesOpt(x_train, y_train)
```
- objective, metric, learing_rateëŠ” ì§€ì • í•´ì£¼ëŠ” ê²ƒì´ ì¢‹ìŒ

```python
# â–¶ lightGBM
from lightgbm import LGBMClassifier

# â–¶ setting the parameters
LGBM = LGBMClassifier(n_estimators=100, max_depth=2)
LGBM.fit(x_train, y_train)


# â–¶ ì˜ˆì¸¡
y_pred_train = LGBM.predict(x_train)
y_pred_test = LGBM.predict(x_test)

y_pred_train_proba = LGBM.predict_proba(x_train)[:, 1] 
y_pred_test_proba = LGBM.predict_proba(x_test)[:, 1] 

lgbm_re = pd.DataFrame({ 'model' : ['LGBM(BO)3'],
                      'f1_train' :  metrics.f1_score(y_train,y_pred_train),
                      'f1_test' : metrics.f1_score(y_test,y_pred_test),
                      'AUC_train' : roc_auc_score(y_train, y_pred_train_proba),
                      'AUC_test' : roc_auc_score(y_test, y_pred_test_proba),}
                    )

df_comparison = df_comparison.append(lgbm_re)
df_comparison.reset_index(drop=True, inplace = True)
```
- n_est:163, max depth:2

```python
df_comparison
```
- overfittingì´ ìƒê°ë³´ë‹¤ëŠ” ì˜ ì•ˆ ì¡í˜..

```python
df_comparison.style.background_gradient(cmap='coolwarm', low=1)
```

![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240114132946.png?raw=true)
â–¶ğŸ’¯ **Best Model & Hyper parameter** > LGBMClassifier(n_estimators=100, max_depth=2)  

## **02. Model explanation**
---

> **Permutation Importance** 

*  Featureì™€ ì‹¤ì œ ê²°ê³¼ê°’ ê°„ì˜ ê´€ê³„(ì—°ê²°ê³ ë¦¬)ë¥¼ ëŠì–´ë‚´ë„ë¡ íŠ¹ì„±ë“¤ì˜ ê°’ì„ ëœë¤í•˜ê²Œ ì„ì€ í›„ ëª¨ë¸ ì˜ˆì¸¡ì¹˜ì˜ ì˜¤ë¥˜ ì¦ê°€ëŸ‰ì„ ì¸¡ì •,  
- ë§Œì•½ í•˜ë‚˜ì˜ íŠ¹ì„±ì„ ë¬´ì‘ìœ„ë¡œ(shuffle) ì„ì—ˆì„ ë•Œ ëª¨ë¸ ì˜¤ë¥˜ê°€ ì¦ê°€í•œë‹¤ë©´ ëª¨ë¸ì´ ì˜ˆì¸¡í•  ë•Œ í•´ë‹¹ íŠ¹ì„±ì— ì˜ì¡´í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸
* ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ë©´, ì¤‘ìš”í•œ Feature
* ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ê·¸ëŒ€ë¡œì´ê±°ë‚˜, ì¢‹ì•„ì§€ë©´ ì¤‘ìš”í•˜ì§€ ì•Šì€ Feature

> **Shapley Value** 

* íŠ¹ì • Featureê°€ ì˜ˆì¸¡ê°’ì— ì–¼ë§Œê°€ ê¸°ì—¬í•˜ëŠ”ì§€ íŒŒì•…í•˜ê¸° ìœ„í•´ íŠ¹ì • ë³€ìˆ˜ì™€ ê´€ë ¨ëœ ëª¨ë“  ë³€ìˆ˜ ì¡°í•©ë“¤ì„ ì…ë ¥ì‹œì¼°ì„ ë•Œ ë‚˜ì˜¨ ê²°ê³¼ê°’ê³¼ ë¹„êµë¥¼ í•˜ë©´ì„œ ë³€ìˆ˜ì˜ ê¸°ì—¬ë„ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ì‹, ì¦‰ íŠ¹ì • Feature(on/off)ê°€ ì˜ˆì¸¡ê°’ì— ì–¼ë§ˆë‚˜ ì˜í–¥ì„ ë¼ì³¤ëŠ”ì§€ íƒìƒ‰

```python
# â–¶ ì„¤ì¹˜
!pip install eli5
```

> ### **â”” Permutation Importance** 

```python
import eli5
from eli5.sklearn import PermutationImportance

# â–¶ LR Model
perm = PermutationImportance(LR, random_state=1).fit(x_train_sc, y_train)
eli5.show_weights(perm, feature_names = x_train.columns.tolist())
```
- random state ê³ ì • í•„ìˆ˜
![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240114133132.png?raw=true)

```python
# â–¶ RFC Model
perm = PermutationImportance(rfc, random_state=1).fit(x_train, y_train)
eli5.show_weights(perm, top = 11, feature_names = x_train.columns.tolist())
```

![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240114133159.png?raw=true)

```python
# â–¶ LGBM Model
perm = PermutationImportance(LGBM, random_state=3).fit(x_train, y_train)
eli5.show_weights(perm, top = 11, feature_names = x_train.columns.tolist())
```
- total_amt,cntê°€ ì¤‘ìš” featureë¡œ íŒë‹¨
![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240114133224.png?raw=true)

> ### **â”” SHAP**

```python
# â–¶ shap
!pip install shap
```

```python
import shap
shap.initjs()
# â–¶ LR shape
explainer = shap.LinearExplainer(LR.fit(x_train_sc, y_train), x_train_sc)
shap_values = explainer.shap_values(x_test_sc)
shap.summary_plot(shap_values, x_test_sc, feature_names=x_test.columns, plot_type="bar",class_names=y_test)
```
- LRì˜ ê²½ìš° linearexplainerê¼­ ë„£ì–´ì¤˜ì•¼ í•¨ 
![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240114133518.png?raw=true)

```python
# â–¶ LR shap
shap.summary_plot(shap_values, x_test_sc, feature_names=x_test.columns)
```

![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240114133542.png?raw=true)

```python
# â–¶ LR shap
shap.initjs()
shap.force_plot(explainer.expected_value, shap_values[0], features = x_test_sc[0], feature_names = x_test.columns, link='logit')
```
- ê° ê°ì²´ì— ëŒ€í•œ ì˜í–¥ë„ í™•ì¸ í•´ ë³¼ ìˆ˜ ìˆìŒ
- link='logit'ì€ ê¼­ ë„£ì–´ì¤˜ì•¼ í•¨ ì£¼ì˜
![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240114133637.png?raw=true)

```python
import shap
shap.initjs()
# â–¶ RFC shap
explainer = shap.TreeExplainer(rfc)
shap_values = explainer.shap_values(x_test)
shap.summary_plot(shap_values, x_test, feature_names=x_test.columns, plot_type="bar")
```

![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240114133832.png?raw=true)

```python
# â–¶ RFC shap
shap.summary_plot(shap_values[1], x_test)
```

![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240114133853.png?raw=true)

```python
# â–¶ RFC shap
shap.initjs()
shap.force_plot(explainer.expected_value[1], shap_values[1][0,:], feature_names = x_test.columns, link='logit')
```

![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240114133945.png?raw=true)

```python
import shap
shap.initjs()
# â–¶ LGBM shap
explainer = shap.TreeExplainer(LGBM)
shap_values = explainer.shap_values(x_test)
shap.summary_plot(shap_values, x_test, feature_names=x_test.columns, plot_type="bar")
```

![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240114134019.png?raw=true)

```python
# â–¶ LGBM shap
shap.summary_plot(shap_values[1], x_test)
```

![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240114134037.png?raw=true)

```python
# â–¶ RFC shap
shap.initjs()
shap.force_plot(explainer.expected_value[1], shap_values[1][4,:] , x_test.iloc[4,:], link='logit')
```

![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240114134241.png?raw=true)

```python
import shap
shap.initjs()
# â–¶ LGBM shap
explainer = shap.TreeExplainer(LGBM)
shap_values = explainer.shap_values(x_test)
shap.summary_plot(shap_values, x_test, feature_names=x_test.columns, plot_type="bar")
```

![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240114134709.png?raw=true)

```python
# â–¶ LGBM shap
shap.summary_plot(shap_values[1], x_test)
```

![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240114134725.png?raw=true)

```python
# â–¶ RFC shap
shap.initjs()
shap.force_plot(explainer.expected_value[1], shap_values[1][4,:] , x_test.iloc[4,:], link='logit')
```

![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240114134639.png?raw=true)

## **03. Model operation**
---
> **ìµœì¢… ëª¨ë¸ í™œìš© ìš´ì˜ ì¤€ë¹„**

1. Target ê³ ê° ìˆ˜ì¤€ ê²°ì • : ìµœì¢… Selectionëœ Model ê¸°ì¤€ìœ¼ë¡œ ì¬êµ¬ë§¤ ê°€ëŠ¥ì„±ì´ ë†’ì€ Target ê³ ê°êµ° ì„ ì •
2. Model Save : Model operationì„ ìœ„í•´ ëª¨ë¸ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥ í›„ ìš´ì˜ì‹œ Model ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° load
3. Mart ìƒì„± : Martë¥¼ ìƒì„±í•  ë•Œ ì‚¬ìš©í–ˆë˜ ì „ì²˜ë¦¬ ë° ì™„ë£Œ ì½”ë“œë¥¼ ì €ì¥ í›„ ìš´ì˜ì‹œ ë¶ˆëŸ¬ì™€ ì›” ë³„ ìƒˆë¡œìš´ Mart ìƒì„±
4. Model Input : ìƒì„±ëœ Martë¥¼ Modelì— Inputí•˜ì—¬ ê²°ê³¼ë¥¼ ì¶”ì¶œ (â€» proba ì˜ˆì¸¡í™•ë¥ )
5. Target ì¶”ì¶œ : 1 ë‹¨ê³„ì—ì„œ ì„ ì •í•œ threshold ê¸°ì¤€ìœ¼ë¡œ Target ê³ ê° ì¶”ì¶œ

```python
# â–¶ model LGBM
df_comparison
```
- best modelì€ LGBM(BO)3

```python
# â–¶ model LGBM
LGBM = LGBMClassifier(n_estimators=100, max_depth=2)
LGBM.fit(x_train, y_train)

# â–¶ ì˜ˆì¸¡
y_pred_train = LGBM.predict(x_train)
y_pred_test = LGBM.predict(x_test)

y_pred_train_proba = LGBM.predict_proba(x_train)[:, 1]
y_pred_test_proba = LGBM.predict_proba(x_test)[:, 1]
```
- best model ì ìš©

```python
# â–¶ train target
import numpy as np

bins=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
lift_base = y_train.value_counts(normalize=True)[1]

# â–¶ bin ê³ ì • scoring /  [ : í¬í•¨, ) : í¬í•¨X
confusion_matrix1 = pd.crosstab(pd.cut(y_pred_train_proba, bins, right=False), y_train , rownames=['Predicted'], colnames=['Actual'], margins=True)

# confusion_matrix1 = pd.crosstab(pd.qcut(y_pred_train_proba, 10), y_train , rownames=['Predicted'], colnames=['Actual'], margins=True)
confusion_matrix1['ratio']=round((pd.DataFrame(confusion_matrix1)[1]/pd.DataFrame(confusion_matrix1)['All']),2)
confusion_matrix1['Lift']=round(confusion_matrix1['ratio']/lift_base,1)
confusion_matrix1
```
- ì˜ˆì¸¡ í™•ë¥ ì„ 10ë“±ë¶„ìœ¼ë¡œ ë‚˜ëˆ  ê°€ì§€ê³  ê±°ê¸°ì— í•´ë‹¹í•˜ëŠ” íƒ€ê²Ÿ ê³ ê° êµ°ë“¤ì˜ ì§€í‘œ í™•ì¸ 
- ì¬êµ¬ë§¤ìœ¨ ì •í™•ì„± í™•ì¸: lift base -> train dataì—ì„œì˜ ì¬êµ¬ë§¤ìœ¨ ê°€ì ¸ì˜´(38%)
- pd.crosstab(pd.qcut(y_pred_train_proba, 10)-> ì–˜ëŠ” ì•Œì•„ì„œ ì ì ˆíˆ 10ë“±ë¶„ í•´ì¤Œ 
	 ë‘˜ ì¤‘ í•˜ë‚˜ ê³¨ë¼ ì”€

```python
# â–¶ test target
# bins=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
bins=[0.0, 0.5, 1.0]
lift_base = y_test.value_counts(normalize=True)[1]

confusion_matrix1 = pd.crosstab(pd.cut(y_pred_test_proba, bins, right=False), y_test , rownames=['Predicted'], colnames=['Actual'], margins=True)
confusion_matrix1['ratio']=round((pd.DataFrame(confusion_matrix1)[1]/pd.DataFrame(confusion_matrix1)['All']),2)
confusion_matrix1['Lift']=round(confusion_matrix1['ratio']/lift_base,1)
confusion_matrix1
```
- binì„ ì €ë ‡ê²Œ 2ë“±ë¶„í•˜ë©´ 0.5 ì´í•˜, ì´ˆê³¼ ì´ë“±ë¶„ í˜•íƒœë¡œ ë³´ê¸° ì¢‹ìŒ

```python
import pickle
# â–¶ ëª¨ë¸ ì €ì¥
saved_model = pickle.dumps(LGBM)

# â–¶ ëª¨ë¸ Read
LGBM = pickle.loads(saved_model)
```

-----------------------EOD---------------------------
