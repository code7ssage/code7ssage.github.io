---
layout: post
title: KNN
---

KNN은 "K-Nearest Neighbors"의 약자로, 지도 학습 방식([[2024-03-25-Supervised learning]])의 머신러닝 알고리즘 중 하나입니다. KNN은 분류(classification)와 회귀(regression) 문제에 모두 사용될 수 있으며, 그 작동 원리는 매우 직관적입니다.

### KNN의 기본 원리:

1. **거리 측정**: KNN은 입력 데이터 포인트와 학습 데이터 세트의 각 포인트 사이의 거리를 측정합니다. 거리 측정 방법으로는 유클리디안 거리, 맨해튼 거리 등이 일반적으로 사용됩니다.
    
2. **K의 선택**: 'K'는 고려할 이웃의 수를 의미합니다. 예를 들어, K=3이라면 가장 가까운 3개의 이웃을 고려합니다.
    
3. **다수결 또는 평균**: 분류 문제의 경우, 가장 가까운 K개의 이웃 중 다수결(majority vote)을 통해 클래스를 결정합니다. 회귀 문제의 경우, 이웃들의 출력 값의 평균을 사용하여 예측값을 결정합니다.
    

### 장점:

- 이해하고 구현하기 쉽습니다.
- 훈련 단계가 다른 알고리즘에 비해 간단합니다(데이터를 저장하는 것이 전부입니다).
- 분류와 회귀에 모두 사용할 수 있습니다.

### 단점:

- 데이터 세트가 커지면 예측 단계에서 많은 계산이 필요하므로 속도가 느려질 수 있습니다.
- 특성의 수가 많을 경우(차원의 저주), 성능이 저하될 수 있습니다.
- 모든 특성이 같은 중요도를 가진다고 가정하므로, 중요도가 다른 특성이 있을 경우 성능에 영향을 미칠 수 있습니다.