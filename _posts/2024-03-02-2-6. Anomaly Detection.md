---
title: 2 6. Anomaly Detection
categories:
  - machine_learning
toc: true
---

# Anomaly Detection 기초
- 이상치(Outlier) vs 이상(Abnormal)
	- **이상치(Outlier)** → Data Noise (O)! 
		- 관측된 데이터의 범위에서 많이 벗어난 아주 작은 값이나, 큰 값
		- 분석하고자 하는 데이터에서 적은 확률로 나타나는 데이터
		- 분석 결과 해석 시 오해를 발생시킬 수 있기 때문에 사전 제거 필요
		- 분석 Domain에 따라 다르지만 대부분 분석 대상이 아님
	- **이상(Abnormal)** → Data Noise (X)!
		- 문제 해결의 관점으로 바라볼 수 있음
		- 현업의 Domain의 관점에서 보았을 때, 문제 발생 가능성이 높은 데이터
		- 정상적인 범주에 데이터라도 이상으로 정의할 수 있음
		- 일반적으로 자주 발생하지 않는 패턴이 이상일 확률이 높음
		- 따라서, 데이터에 대해 Thresh Hold가 존재함
		![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240112125458.png?raw=true)

- Data 유형에 따른 이상 탐지의 종류
	1. Point Anomaly Detection
		- 축적된 시간 동안 정적인 점 분포에 초점
		- 특정 Point의 이상치를 감지
		- 일반적으로 말하는 Data 내 Outlier
	2. Contextual Anomaly Detection
		- 시계열과 같은 동적인 특성에 초점 (※ 과거에 데이터가 현재의 값에 영향을 미칠 때, Sequential)
		- 연속적인 변화 패턴을 읽어 이상치 감지, 맥락을 고려해서 예상 변화와 동떨어진 결과를 탐지함
		- 민감하면 정상상황에서도 이상탐지가 되고, 둔감하게 만들면 비정상 상황에서 이상 탐지를 놓칠 수 있음
	3. Collective Anomaly Detection
		- Contextual Anomaly Detection와 다르게 Global 상황에서 변칙적인 이상치가 아닌, Local한 이상치
		- 개별 데이터가 아닌 집합 데이터 비교를 통해 이상을 확인함
	4. Online Anomaly Detection
		- 실시간 데이터 수집 체계가 구축되어 있는 환경에서 탐지
			: 실시간은 몇 분 혹은 몇 초 마다 데이터가 수집되어야 실시간이라고 볼 수 있는가?
			→ Domain에 따라 다름
		- 실시간 데이터를 어떻게 빠르게 처리하고 이상을 탐지할지 설계하는 것이 매우 중요함
	5. Distributed Anomaly Detection
		- 관측치의 정상 분포에서 벗어나는 이상 데이터를 탐지

Anomaly Detection 종류 - **Label**
1. **Supervised Anomaly Detection**
	- 모든 관측치에 대해서 이상(abnormal)과 정상(normal) 라벨이 붙어있는 경우, 자주 발생하지 않는
	이벤트인 이상(abnormal)에 대해서 정의할 수 있는 상황이며, 충분히 학습할 만한 이상(abnormal) 데이터가 확보되어 있는 상황
	- 일반적인 이진분류(Binary Classification) 문제로 해결할 수 있음
	(※ 이상(abnormal)의 종류가 다양하다면 Multi-Classification 문제로 해결)
	- 이상(abnormal) 데이터 확보가 가능하나 적은 수의 데이터가 존재할 확률이 있으므로, 
	소수 클래스 문제, 불균형 문제를 해결해야 하는 상황을 고려할 수 있음
	- 성능평가 시 ACC(Accuracy) 관점이 아닌, Recall 관점에서의 성능평가도 고려해야 함
	이상(abnormal) 데이터가 매우 적기 때문에 ACC는 높게 산출될 가능성이 큼
	- [장점 : 양/불 판정 정확도가 높다, 단점 : Class-Imbalance 문제]
2. **Semi-Supervised Anomaly Detection**
	- 이상(abnormal)에 대한 라벨이 없으며, 정상(normal) 데이터만 보유하고 있는 상황, 충분히 학습할
	만한 이상(abnormal) 데이터가 없을 시 정상(normal) 데이터만을 사용하여 이상(abnormal)을 탐지
	- 정상(normal) 데이터의 패턴을 학습하여 이상(abnormal) 데이터가 들어올 시 정상(normal) 데이터와
	차이점을 파악하여, 이상(abnormal) 데이터를 탐지
	- 정상 데이터를 둘러싸고 있는 discriminative boundary(구분선)를 설정하고, 이 boundary를 최대한
	좁혀 밖에 있는 데이터들을 이상(abnormal)로 간주하는 것
	- 대표적인 방법으로는 One-Class SVM이 존재
	- [장점 : 연구가 활발하게 이뤄지고 있고, 정상(normal) 데이터만 있어도 학습이 가능 ,                    단점 : Supervised Anomaly Detection 방법론 대비 양/불 판정 정확도가 떨어짐]
3. **Unsupervised Anomaly Detection**
	- 정상(normal)과 이상(abnormal)에 대한 라벨이 모두 존재하지 않는 경우
	- 모든 데이터를 활용하여 학습
	- 일반적으로 딥러닝 알고리즘을 사용하여 정상 패턴을 학습하고, 학습된 모델을 통해 이상(abnormal) 데이터를 탐지함
	- [장점 : 라벨링 과정이 필요 없음, 단점 : 양/불 판정 정확도가 높지 않고 hyper parameter에 매우 민감함]

- **3-Sigma Rule
	- SPC(Statistical Process Control, 통계적 공정 관리), 경험적 규칙(Empirical Rule)
	- 표준편차의 3배인 3-Sigma (б)의 범위가 전체 Data의 99.73%를 포함
	- Data 전체의 산포를 파악하는 개념으로 많이 활용 됨
	- UCL (Upper Control Limit) / LCL (Lower Control Limit)
		- 6 б 수준 - 불량률 & 불량 개수 : 0.00034% & 100만개 중 3.4개
		- 3 б 수준 - 불량률 & 불량 개수 : 6.7% & 100만개 중 66,807개
		![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240112130404.png?raw=true)
	- [good]: 매우 구현하기 쉬움, 명확한 기준을 설정할 수 있기 때문에, 방법론에 재학습이 필요 없음 
	- [bad]:  정규분포가 가정되어야 함, 분포에 따라 б 의 크기 수준이 달라 짐, 다차원 데이터에 적용하기 어려움, 단변량 데이터에 적용할 수 있으며, Feature간 상호관계를 파악하기 어려움

- **Box Plot**
	- 많은 데이터를 눈으로 확인하기 어려울 때 그림을 이용해 데이터 집합의 범위와 중앙 값을 빠르게 확인할 수 있음
	- 데이터가 많은 경우 분포를 그리기 매우 힘들 수 있기 때문
	- Box plot은 최소 정상 값(min), 1사분위수(Q1), 중앙값, 3사분위수(Q3), 최대 정상 값(max)를 나타내는 시각화 방법
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240112130638.png?raw=true)
	- [good]: 데이터를 눈으로 확인하기 어려울 때, 그림을 이용해 데이터의 범위를 빠르게 파악, 통계적으로 이상치(Outlier)가 있는지 확인 가능, 다양한 정보를 손쉽게 표현할 수 있음 
	- [bad]: 위치 정보에 기반하고 있기 때문에 분포를 확인하기는 어려움, 가운데 선은 평균이 아니다. 오해 소지가 있음
- 실습: [21. sigma Rule & Box plot Code](https://code7ssage.github.io/code_file/21.-sigma-Rule-&-Box-plot-Code/)

# LOF
- **L**ocal **O**utlier **F**actor
- 대부분의 이상치 탐지 알고리즘은 전체 데이터와 비교하여 각각의 관측치가 이상인지 정상인지 판단함
- 아래 그림의 경우 𝑶𝟐에 대해 이상치라고 판단하지 않음
	- 𝑪𝟏은 밀도가 낮은 그룹, 𝑪𝟐는 밀도가 높은 그룹
	- 𝑶𝟏, 𝑶𝟐 는 이상치

- 𝑪𝟐와 𝑶𝟐사이의 거리는 𝑪𝟏 그룹 내 관측치 간의 거리와 유사하기 때문에 전체적인 데이터 관점에서 보면 𝑶𝟐는 이상치로 보기 어려움
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240112131144.png?raw=true)

- k- distance of an object p : 
	: 𝑘 − 𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝑝) : 관측치 p와 k번째로 가까운 이웃의 거리
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240112131402.png?raw=true)

- k- distance neighborhood of an object p 
	: 𝑵𝒌(𝒑) : 관측치 p의 k- distance(p) 보다 가까운 이웃의 집합, 관측치 p를 중심으로 k-distance로 원을 그릴 때 원안에 있는 point 수
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240112131459.png?raw=true)

- Reachability distance of an object p w.r.t object o 
	- B는 3-distance와 같으니 A와 B의 Distance는 똑같음 
	- C는 3-distance보다 작으니 3-distance 값으로 함 
	- D는 3-distance보다 크니 A와 D의 Distance 값으로 함
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240112131705.png?raw=true)

- Local reachability density of an object p 
	- 관측치 p에 대한 k-이웃의 RD(p, o) 평균의 역수 
	- 관측치 p 주변에 이웃이 얼마나 밀도 있게 있는 가를 대변하는 값임
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240112131953.png?raw=true)

- Local outlier factor (LOF) of an object p 
	- 관측치 p의 이상치 정도를 나타내는 LOF는 p의 밀도와 이웃 o의 밀도의 비율을 평균한 것 
	- p의 밀도가 작을수록, o의 밀보다 클수록 LOF 값은 커지며, 이상치라고 판단할 수 있음
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240112131838.png?raw=true)
- 판단?
	- LOF < 1 : 밀도가 높은 분포 
	- LOF ≒ 1 : 이웃 관측치와 비슷한 분포 
	- LOF > 1 : 밀도가 낮은 분포, 크면 클수록 이상치 확률이 높음
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240112132312.png?raw=true)
- 실습: [22. LOF Code](https://code7ssage.github.io/code_file/22.-LOF-Code//)

# Isolation Forest
- 기본적으로 의사결정나무(Decision Tree) 형태로 표현해 정상 값을 분리하기 위해서는 의사결정나무를 깊숙하게 타고 내려 가야 하고, 반대로 이상치인 경우 의사결정나무 상단부에서 분리할 수 있다는 것을 이용한 기법임
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240112132453.png?raw=true)

- Random Forest 처럼 Decision Tree를 앙상블 하듯, Isolation Forest도 마찬가지로 Decision Tree를 앙상블함 
	- Sub-sampling : 비복원 추출하여 Tree에 들어갈 Dataset을 준비함 
		: Random Forest의 경우 복원 추출을 함 (약 37% 데이터가 뽑히지 않기 때문에 Data Noise에 강건해질 수 있었음) 
	- Feature Randomly Selection : 변수를 랜덤하게 선택함
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240112132601.png?raw=true)

- Split point 설정
	- 선택된 변수들의 값을 랜덤하게 분기하기 위해 해당 변수의 최소/최대 값 사이에 정의된 Uniform 분포에서 샘플링 진행
	- 사용하는 데이터의 크기 만큼 나무의 최대 깊이가 𝒍𝒐𝒈𝟐N만큼 정해지므로 (데이터가 256개라면 최대 깊이 7)
	- 최대 깊이에 도달할 때까지 지속적으로 Split 진행 (모든 데이터 고립 → **Fully overfitting!!**)

- 각 데이터 마다의 Scoring 계산
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240112132912.png?raw=true)
	- 𝒉(𝒙) : 해당 관측치의 경로 길이 (Root Node로 부터의 거리, Depth)
	- 𝑬(𝒉(𝒙)) : 모든 iTree에서 해당 관측치에 대한 경로 길이 평균
		예를 들어, Tree가 100개라면 그 중 해당 데이터가 있는 Tree의 𝒉(𝒙) 계산 후 평균
	- 𝒄(𝒙) : iTree의 평균 경로 길이 (Tree마다 Score 값을 Normalize 하기 위함)
	- Score는 0 ~ 1 사이 값을 가지며 1에 가까울 수록 이상치 일 확률이 높고 0.5 이하면 정상 데이터로 판별
		- 𝑬 𝒉 𝒙 = 𝒄(𝒏) 인 경우 Score는 0.5
		- 𝑬 𝒉 𝒙 = 𝟎 인 경우 Score는 1
		- 𝑬 𝒉 𝒙 = 최대경로 인 경우 Score 0 

- 예시 : Tree Height limit 
	- Tree Depth : 위에 있는 노드의 수
	- Tree height : 아래 있는 노드의 수
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240112133107.png?raw=true)
- 실습: [23. Isolation Forest Code](https://code7ssage.github.io/code_file/23.-Isolation-Forest-Code//)

# Robust Random Cut Forest
- Robust Random Cut Forest 가정은 이상치는 학습하는 **Model의 복잡성을 증가 시킴** 
	: 다른 데이터 들의 **Depth가 증가할 것 임**
- Robust Random Cut Forest에서 그림의 Data 'x'를 제거 하면 subtree c 안에 있는 노드들이 depth가 모두 1씩 감소함 
- 한편 subtree b의 depth는 바뀌지 않음. 즉, 'x'를 제거하면 subtree c에 있는 data들의 depth가 1씩 감소 
- 따라서, 'x'의 자매 노드에 있는 data 개수가 depth 변화의 총합이 됨 
- 'x' data가 추가 될 경우, 어떤 data들은 path length가 증가함 : 이것을 Score로 사용 → Displacement (Disp)
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240112141511.png?raw=true)

- Disp Score의 경우 **masking** 문제에 취약함 
- 이상 Data끼리 서로 뭉쳐 있으면 그 이상 데이터를 제거 하더라도 Disp는 1 정도 밖에 되지 않을 것 
- 따라서 colluder(공모자)들까지 고려하는 이상스코어 **Collusive Displacement (CoDisp)** 제안 
- 'x'의 공모자들의 집합을 C라고 가정 (C는 우리가 알 수 없는 것이고, x를 포함하는 가능한 모든 부분 집합을 고려함) 
	: CoDisp 값이 크면 클수록 이상치로 간주함

- IF와 비교
	- positive recall이 더 높음
	- 시간이 절반 밖에 안걸림
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240112141753.png?raw=true)
- 실습: [24. Robust Random Cut Forest Code](https://code7ssage.github.io/code_file/24.-Robust-Random-Cut-Forest-Code//)

# One-class SVM
- Support Vector Machine 
- SVM(Support Vector Machine) : Deep Learning 이전 뛰어난 성능으로 많이 주목 받았던 Algorithm 
- 지도 학습이며 [Classification](https://code7ssage.github.io/key_terms/classification/) Problem에 사용 됨
- linear functions임
	- Decision Boundary를 그릴 때 정확도가 100%라도 **Data 분포에 따라 합리적으로 “잘” 그릴 수 있도록 하는 방법**
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240112142147.png?raw=true)
- **Margin**을 Maximize 하는 Decision Boundary를 찾아 보자 !
	- b12 : minus-plane 
	- b11 : plus-plane 
	- b11 – b12 = Margin(마진) → 최적화 → SVM
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240112142246.png?raw=true)

- Hard margin: 마진 감소 -> narrow
- Soft margin: 마진 증가 -> wide
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240112142600.png?raw=true)

- 비선형 문제는? -> Kernel로 데이터를 고차원으로!
	- 그러면서 서로 분리 되는 경계를 찾고 Decision Boundary를 그려 주면 됨
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240112142741.png?raw=true)

- One-class Support Vector Machine for Anomaly Detection
	- 정상 관측치가 최대한 원점에서 멀리 떨어지도록 학습하는 것 
	- P : 원점에서 분류 경계면까지의 거리
	- Data를 Kernel을 활용하여 space로 뿌려주고, normal과 abnormal의 분류 경계면을 통해 mapping 된 data 중 normal data들이 원점으로 부터 최대한 멀어지게 만드는 것
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240112142948.png?raw=true)
- 실습: [25. One-class SVM Code](https://code7ssage.github.io/code_file/25.-One-class-SVM-Code//)

# Dimensionality Reduction 기법 활용한 이상치 탐지
- Anomaly Detection using **PCA** 
	- Original data → Dimensionality Reduction → Reconstruction 
	- **Original data 와 Reconstruction Data의 거리를 계산하여 Anomaly을 측정함** 
	- Data 전체의 분산을 보존 : 정상 Data의 패턴을 파악해서 Reduction을 했다고 판단 
	- 그 정보를 바탕으로 정상적인 고차원으로 Reconstruction 후 Original data와 비교 
	- 정상 범주는 Original data와 차이가 별로 없을 것, 이상 범주는 Original data와 차이가 크게 날 것
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240112143152.png?raw=true)

- Anomaly Detection using **Autoencoder** 
	- 1. 정상 Data 만을 학습하여 Reconstruction 하면 이상 Data가 들어왔을 때 Error가 크게 발생할 것 
		- 정상적인 Pattern만 학습 했을 것이기 때문 
		- Error를 바탕으로 Threshold를 잡아 정상 vs 이상 판단 
	- 2. Latent vector를 1 Dimension으로 설정 후 1 Dimension 값을 활용하여 3-Sigma 분석 
		- 정상 Data는 특정 영역에서 값을 도출할 것이며, 이상 Data는 3-Sigma에서 벗어난 값으로 도출 될 확률이 높음
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240112143436.png?raw=true)

- Anomaly Detection using **DBSCAN**
	- DBSCAN은 Clustering 기법 중 하나임 
	- 하지만 Noise Data는 어느 Class로 분류해주지 않기 때문에 이상치로 판단할 수 있음 
	- DBSCAN을 활용하여 Noise Data를 판별하고 정상 내에서도 다양한 정상 군집들을 발견할 수 있음
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240112143451.png?raw=true)

- Anomaly Detection using Hierarchical **Clustering** 
	- Isolation Forest와 비슷한 원리로 덴드로그램 상단부에 소수의 데이터가 분리 되어 나가면 이상치라고 판단할 수 있음 
	- 이상치도 약간의 비슷한 패턴을 가지고 있는 소수 Data에 잘 Working 함 
	- Hierarchical Clustering을 활용하여 Noise Data를 판별하고 정상 내에서도 다양한 정상 군집들을 발견할 수 있음
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240112143555.png?raw=true)
	