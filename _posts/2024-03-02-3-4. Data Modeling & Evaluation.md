---
title: 3 4. Data Modeling & Evaluation
categories:
  - machine_learning
---

# **ğŸš© Chapter 04**  

---
> **ëª©ì°¨(Context)**

* 01.ëª¨ë¸ë§ì„ ìœ„í•œ ë°ì´í„° ì‚¬ì „ ì¤€ë¹„
* 02.Model Selection
* 03.ëª¨ë¸ë§ ë° ì„±ëŠ¥ ë¹„êµ (1)
* 04.ëª¨ë¸ë§ ë° ì„±ëŠ¥ ë¹„êµ (2)
* 05.Model evaluation

## **01. ëª¨ë¸ë§ì„ ìœ„í•œ ë°ì´í„°ì‚¬ì „ ì¤€ë¹„**
---
> **ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ì˜ í˜•íƒœë¡œ ê°€ê³µ** 

* ëª¨ë¸ì€ ìˆ«ìë¡œ ì´ë£¨ì–´ì§„ í˜•íƒœì˜ Dataë§Œ ì¸ì‹ ê°€ëŠ¥ (â€» ë¬¸ìí˜• ë³€ìˆ˜ ì¸ì½”ë”© í•„ìš”)
* ëª¨ë¸ë§ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ Featureì™€ ì˜ˆì¸¡í•˜ê³ ìí•˜ëŠ” ê°’ì¸ Yë¡œ ë°ì´í„°ë¥¼ ë‚˜ëˆ”
* í•™ìŠµê³¼ ì˜ˆì¸¡ì„ ìœ„í•œ Train / Test set ë¶„í• 

> **ëŒ€í‘œì ì¸ Encodingì˜ ë°©ë²•** 

1. One-hot encoding : ë²”ì£¼í˜•(Categorical) ë³€ìˆ˜ì˜ ìœ í˜• ë§Œí¼ ìƒˆë¡œìš´ Featureë¥¼ ìƒì„±í•˜ê³ , 1ê³¼ 0ìœ¼ë¡œ í‘œí˜„
2. Label encoding : ë²”ì£¼í˜•(Categorical) ë³€ìˆ˜ì˜ uniqueí•œ ê°’ë§Œí¼ ì¦ê°€í•˜ëŠ” ìˆ«ìë¥¼ ë¶€ì—¬

> **â‘  One-hot encoding** 

* ë²”ì£¼í˜• ë°ì´í„°ì˜ uniqueí•œ ê°’ì´ ì¦ê°€í•˜ë©´ ì¦ê°€í•  ìˆ˜ë¡ ë³€ìˆ˜ì˜ ì°¨ì›ì´ ëŠ˜ì–´ë‚˜ëŠ” íš¨ê³¼
* ì˜ˆì¸¡ ì„±ëŠ¥ì˜ ì €í•˜ 
* ë²”ì£¼í˜• ë³€ìˆ˜ì˜ Uniqueí•œ ìˆ˜ì¤€ì„ ì¶•ì†Œí•˜ê³  ì‚¬ìš©í•˜ê¸°ë¥¼ ê¶Œì¥

> **â‘¡ Label encoding** 

* Uniqueí•˜ê³  ì¦ê°€í•˜ëŠ” ìˆ«ìê°’ìœ¼ë¡œ ë³€í™˜ë˜ì–´ ìˆ«ìì˜ ordianlí•œ íŠ¹ì„±ì´ ë°˜ì˜ë˜ì–´ ì˜ë„í•˜ì§€ ì•Šì€ ê´€ê³„ì„±ì´ ìƒê¹€
* ìˆ«ìê°’ì„ ê°€ì¤‘ì¹˜ë¡œ ì˜ ëª» ì¸ì‹í•˜ì—¬ ê°’ì— ì™œê³¡ì´ ìƒê¹€
* ì˜ˆì¸¡ ì„±ëŠ¥ì˜ ì €í•˜
* ì„ í˜•íšŒê·€ì™€ ê°™ì€ ML ì•Œê³ ë¦¬ì¦˜ì—ëŠ” ì ìš©í•˜ì§€ ì•ŠìŒ
* **Tree ê³„ì—´**ì˜ ì•Œê³ ë¦¬ì¦˜ì€ ìˆ«ìì˜ ordinalí•œ íŠ¹ì„±ì„ ë°˜ì˜í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì§„í–‰ ê°€ëŠ¥

![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240114131959.png?raw=true)

```python
# â–¶ ìµœì¢… ì¶”ì¶œëœ Mart read
import pandas as pd
df_mart = pd.read_csv('df_mart.csv')
df_mart.head()
```

```python
# â–¶ í•™ìŠµì— í•„ìš”ì—†ëŠ” Column ì œê±°
df_mart = df_mart.drop(['bsym', 'CustomerID', 'total_amt_grp', 'n_target', 'grp'], axis=1)
df_mart.head()
```

```python
# â–¶ ëª¨ë¸ë§ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ Feature(X)ì™€ Target(Y)ë°ì´í„°ë¥¼ êµ¬ë¶„í•˜ëŠ” ë‹¨ê³„ 
from sklearn.model_selection import train_test_split

X = df_mart.drop(['target'], axis=1)
Y = df_mart['target']

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, stratify=Y, random_state=1234)

print(x_train.shape)
print(y_train.shape)

print(x_test.shape)
print(y_test.shape)
```
- x,y split
- stratify=Y: ê· ë“±í•œ ë¶„ë¥˜ ê°€ëŠ¥, ë„£ì–´ì£¼ë©´ ì¢‹ìŒ

```python
# â–¶ ìˆ«ìí˜•(Integer), ë²”ì£¼í˜•(Categorical) ë³€ìˆ˜ ë¶„í• 
numerical_list=[]
categorical_list=[]

for i in df_mart.columns :
  if df_mart[i].dtypes == 'O' :
    categorical_list.append(i)
  else :
    numerical_list.append(i)

print("categorical_list :", categorical_list)
print("numerical_list :", numerical_list)
```
- OëŠ” object-> categorical_listì— append
- ë‚˜ë¨¸ì§€ëŠ” numerical

```python
# â–¶ (1) One hot encoding

from sklearn.preprocessing import OneHotEncoder

for col in categorical_list :
  encoder = OneHotEncoder()
  encoder.fit(x_train[col](https://code7ssage.github.io/col//))
  onehot_train = pd.DataFrame(encoder.transform(x_train[col](https://code7ssage.github.io/col//)).toarray(), columns = encoder.get_feature_names(), index=x_train.index)
  onehot_test = pd.DataFrame(encoder.transform(x_test[col](https://code7ssage.github.io/col//)).toarray(), columns = encoder.get_feature_names(), index=x_test.index)
  # ê¸°ì¡´ Colì€ ì‚­ì œ
  x_train = pd.concat([x_train,onehot_train], axis = 1).drop(columns = [col])
  x_test = pd.concat([x_test,onehot_test], axis = 1).drop(columns = [col])
```
- fitì€ í•­ìƒ train dataë¡œ
- x_trainì„ 0,1 arrayë¡œ ë³€í™˜, columnì€ featureì—ì„œ ê°€ì ¸ì˜¤ê³ , index(í–‰)ëŠ” ê¸°ì¡´ê³¼ ë™ì¼
- concat: ë‘ columnì„ ë¨¼ì € ë¶™ì´ê³ , drop: ì´í›„ ê¸°ì¡´ column drop

```python
x_train.iloc[0,:], x_test.iloc[0,:]
```

```python
# â–¶ (2) label encoding

from sklearn.preprocessing import LabelEncoder

for col in categorical_list :
  encoder = LabelEncoder()
  encoder.fit(x_train[col](https://code7ssage.github.io/col//))
  
  # ê¸°ì¡´ Col ëŒ€ì²´
  x_train[col] = encoder.transform(x_train[col]) 
  x_test[col] = encoder.transform(x_test[col]) 
```
- columnsì˜ ê°œìˆ˜ê°€ ë°”ë€Œì§€ ì•Šì•„ ë” ê°„ë‹¨

```python
x_train.iloc[0,:], x_test.iloc[0,:]
```

## **02. Model Selection**
---
> **í•™ìŠµí•  Modelë“¤ì„ List-upí•˜ëŠ” ê³¼ì •** 

* ëŒ€í‘œì ì¸ ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ì„œëŠ” ëª¨ë‘ Test í•˜ëŠ” ê²ƒì´ ìœ ë¦¬
* ë¶„ì„í•˜ê³ ìí•˜ëŠ” ë°ì´í„°ì˜ íŠ¹ì„±ì´ ëª¨ë‘ ë‹¤ë¥´ë¯€ë¡œ ì™„ë²½í•œ ì•Œê³ ë¦¬ì¦˜ì€ ì—†ìŒ
* "All models are wrong, but some are useful"
* ìµœì‹  ê°œë°œë˜ê³  ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜ì´ ê°€ì¥ ì„±ëŠ¥ì´ ë†’ì€ ê²ƒì€ ì•„ë‹˜
* Data by Data
* ë™ì¼í•œ ì‘ë™ì›ë¦¬ì˜ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ê¸°ë³´ë‹¤ëŠ”, ë‹¤ë¥¸ ì‘ë™ì›ë¦¬ë¥¼ ê°€ì§€ê³  ìˆëŠ” ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ì„ ì„ ì •

> **Regression** 

* ì„ í˜•íšŒê·€ ëª¨ë¸ (Ridge, Lasso, Elastic Net)
* ë¹„ì„ í˜•íšŒê·€ ëª¨ë¸ (polynomial, logëª¨í˜•)
* Tree ê³„ì—´ Regression ëª¨ë¸
	- bagging ì•™ìƒë¸” (Randomforest)
	- boosting ì•™ìƒë¸” (lightGBM)

> **Classification** 

* ë¡œì§€ìŠ¤í‹± ëª¨ë¸ (logistic regression)
* Tree ê³„ì—´ Classification ëª¨ë¸
	- bagging ì•™ìƒë¸” (Randomforest)
	- boosting ì•™ìƒë¸” (lightGBM)

![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240114132148.png?raw=true)
<ê·¸ë¦¼ - ë² ê¹…, ë¶€ìŠ¤íŒ…>

## **03. ëª¨ë¸ë§ ë° ì„±ëŠ¥ë¹„êµ (1)**
---
> **Model í•™ìŠµ** 

* Model Selection ë‹¨ê³„ì—ì„œ ì„ ì •í•œ ëª¨ë¸ë“¤ì„ í•™ìŠµí•˜ê³  ì„±ëŠ¥ì„ ê¸°ë¡
* ë™ì¼í•œ Data set, ë™ì¼í•œ í™˜ê²½ì—ì„œ ë™ì¼í•œ ë¹„êµ ì§€í‘œë¡œ ì„±ëŠ¥ì„ ë¹„êµ
	- **randomstate** ë™ì¼í•œê±°(=seedê³ ì •)

> **ì„±ëŠ¥í‰ê°€** 

* Regression
	- [MAE](https://code7ssage.github.io/MAE//)(Mean Absolute Errors) : í‰ê· ì ˆëŒ€ì˜¤ì°¨, ë‹¨ìœ„ ìì²´ê°€ ê¸°ì¡´ ë°ì´í„°ì™€ ë™ì¼ 
	- [MSE](https://code7ssage.github.io/MSE//)(Mean Squared Errors) : í‰ê· ì œê³±ì˜¤ì°¨, ì œê³±ìœ¼ë¡œ ì¸í•´ ì›ë˜ì˜ ì˜¤ì°¨ë³´ë‹¤ ë¯¼ê°í•˜ê²Œ ì˜¤ì°¨ê°€ í‘œí˜„ë¨, ì˜¤ì°¨ì˜ ë¯¼ê°ë„ë¥¼ ë†’ì´ëŠ” íš¨ê³¼
	- [RMSE](https://code7ssage.github.io/RMSE//)(Root Mean Square of Errors) : MSEì— Rootë¥¼ ì”Œì–´ ë‹¨ìœ„ë¥¼ ë§ì¶¤
	- R^2(R Squared Score) : ê²°ì •ê³„ìˆ˜, ê´€ì¸¡ê°’ì˜ ë¶„ì‚°ëŒ€ë¹„ ì˜ˆì¸¡ê°’ì˜ ë¶„ì‚°ì„ ë¹„êµí•˜ì—¬ ëª¨ë¸ì˜ ì •í™•ë„ ì„±ëŠ¥ì„ ì¸¡ì •, 0~1ê¹Œì§€ í‘œí˜„
  
* Classification
	- Precision(ì •ë°€ë„) : True(ì •ë‹µ)ì´ë¼ê³  ì˜ˆì¸¡í•œ ê²ƒ ë“¤ ì¤‘ì— ì‹¤ì œ True(ì •ë‹µ)ì¸ ê²ƒì˜ ë¹„ìœ¨
	- Recall(ì¬í˜„ìœ¨, sensitivity) : ì‹¤ì œ True(ì •ë‹µ)ì¸ ê²ƒ ì¤‘ì— ëª¨ë¸ì´ True(ì •ë‹µ)ì´ë¼ê³  ì˜ˆì¸¡í•œ ê²ƒì˜ ë¹„ìœ¨
	- F1-score(ì¡°í™”í‰ê· ) : Precisionê³¼ Recallì˜ ì¡°í™”í‰ê· , ë°ì´í„°ì˜ labelì´ ë¶ˆê· í˜•ì¼ ë•Œ, ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•  ìˆ˜ ìˆìŒ 
	- AUC, AUROC(Area Under ROC Curve) : Xì¶•(FPR, False Positive Rate), Falseì¸ë° Trueë¼ê³  ì˜ ëª» ì˜ˆì¸¡í•˜ ë¹„ìœ¨ / Yì¶•(Recall)

![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240114132229.png?raw=true)

## **04. ëª¨ë¸ë§ ë° ì„±ëŠ¥ë¹„êµ (2)**
---
> **Model í•™ìŠµ** 

```
* Model (1) : Logistic Regression
* Model (2) : Random Forest
* Model (3) : LightGBM
```

> ### **â”” Logistic Regression** 

```python
# â–¶ Logistic Regression í‘œì¤€í™”(standardization) (â€»ì •ê·œí™”(0~1), Normalization)
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
x_train_sc = scaler.fit_transform(x_train)
x_test_sc = scaler.transform(x_test)
```
- íšŒê·€ ëª¨ë¸ì—ì„œ í‘œì¤€í™” vs ì •ê·œí™” ë­ ì“¸ì§€ ì„ íƒí•´ì•¼í•¨
- ì—¬ê¸°ì„œëŠ” í‘œì¤€í™”ë¡œ ì§„í–‰ 

```python
# â–¶ í‘œì¤€í™”ëœ ë°ì´í„° í™•ì¸ 
x_train_sc
```

```python
# â–¶ Model í•™ìŠµ
from sklearn.linear_model import LogisticRegression

LR = LogisticRegression()
LR.fit(x_train_sc, y_train)
```

```python
# â–¶ ê³„ìˆ˜ í™•ì¸
print(LR.coef_)
print(x_train.columns)
```

```python
# â–¶ ê³„ìˆ˜ í™•ì¸
df_coef = pd.DataFrame(x_train.columns, columns=['val'])
df_coef['coef'] = LR.coef_.reshape(-1,1)
df_coef.sort_values(by=['coef'],ascending =False)
```
- ì–‘ì˜ ìƒê´€ ê´€ê³„, ìŒì˜ ìƒê´€ ê´€ê³„ íŒŒì•…

```python
from sklearn.metrics import classification_report

# â–¶ ì˜ˆì¸¡ ë° ì„±ëŠ¥ í™•ì¸
y_pred_train = LR.predict(x_train_sc)
y_pred_test = LR.predict(x_test_sc)

print(classification_report(y_train, y_pred_train))
print(classification_report(y_test, y_pred_test))
```
- classification report: precision, recall, f1 score, support ë“± í™•ì¸ ê°€ëŠ¥
- train, test ë‘˜ ë‹¤ ë³´ê¸°

```python
# â–¶ ê³¼ì í•© ë¬¸ì œ, Trainê³¼ Test setì— ì„±ëŠ¥ì„ ìµœëŒ€í•œ ì¤„ì—¬ì£¼ëŠ” ê²ƒì´ ê³¼ì í•©ì„ ë°©ì§€
from sklearn.metrics import roc_auc_score

y_pred_train_proba = LR.predict_proba(x_train_sc)[:, 1] 
y_pred_test_proba = LR.predict_proba(x_test_sc)[:, 1] 


roc_score_train = roc_auc_score(y_train, y_pred_train_proba)
roc_score_test = roc_auc_score(y_test, y_pred_test_proba)

print("roc_score_train :", roc_score_train)
print("roc_score_test :", roc_score_test)
```
- predict_proba: 0ì— ì†í•  í™•ë¥ , 1ì— ì†í•  í™•ë¥  ì˜ˆì¸¡í•˜ëŠ” 2ê°€ì§€ ì¶œë ¥ 
	[;, 1] ì€ 1ì— ì†í•  í™•ë¥ ë§Œ í‘œì‹œ
- roc_auc_score: yê°’ê³¼ probaë¥¼ ë¹„êµí•˜ì—¬ ì ìˆ˜ë¥¼ ì‚°ì¶œ

```python
# â–¶ ì—¬ëŸ¬ Modelì˜ ì„±ëŠ¥ ë¹„êµë¥¼ ìœ„í•œ ë¹ˆ DataFrame ìƒì„±
df_comparison = pd.DataFrame(columns = {'model', 'f1_train', 'f1_test', 'AUC_train', 'AUC_test'})
df_comparison.columns = ['model', 'f1_train', 'f1_test', 'AUC_train', 'AUC_test']
df_comparison
```

```python
# â–¶ LR modelì— ëŒ€í•œ ì„±ëŠ¥ ê°’ ì¶”ê°€
import sklearn.metrics as metrics

lr_re = pd.DataFrame({ 'model' : ['LR'],
                      'f1_train' :  metrics.f1_score(y_train,y_pred_train),
                      'f1_test' : metrics.f1_score(y_test,y_pred_test),
                      'AUC_train' : roc_auc_score(y_train, y_pred_train_proba),
                      'AUC_test' : roc_auc_score(y_test, y_pred_test_proba),}
                    )

df_comparison = df_comparison.append(lr_re)
df_comparison
```

> ### **â”” Random Forest** 

```python
# â–¶ RandomForest
from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier()
#rfc = RandomForestClassifier(max_depth=2)
rfc.fit(x_train, y_train)

# â–¶ ì˜ˆì¸¡
y_pred_train = rfc.predict(x_train)
y_pred_test = rfc.predict(x_test)

print(classification_report(y_train, y_pred_train))
print(classification_report(y_test, y_pred_test))
```
- ì¼ë‹¨ì€ hyperparameter íŠœë‹ì€ ì•ˆí•˜ê³  ë””í´íŠ¸ ê°’ìœ¼ë¡œ ì§„í–‰
- ê·¼ë° train f1ì´ 1.00ì´ ë‚˜ì˜¤ë„¤? -> overfitting

```python
# â–¶ AUC
y_pred_train_proba = rfc.predict_proba(x_train)[:, 1] 
y_pred_test_proba = rfc.predict_proba(x_test)[:, 1] 

roc_score_train = roc_auc_score(y_train, y_pred_train_proba)
roc_score_test = roc_auc_score(y_test, y_pred_test_proba)

print("roc_score_train :", roc_score_train)
print("roc_score_test :", roc_score_test)
```

```python
# â–¶ RFC modelì— ëŒ€í•œ ì„±ëŠ¥ ê°’ ì¶”ê°€
rfc_re = pd.DataFrame({ 'model' : ['RFC'],
                      'f1_train' :  metrics.f1_score(y_train,y_pred_train),
                      'f1_test' : metrics.f1_score(y_test,y_pred_test),
                      'AUC_train' : roc_auc_score(y_train, y_pred_train_proba),
                      'AUC_test' : roc_auc_score(y_test, y_pred_test_proba),}
                    )

df_comparison = df_comparison.append(rfc_re)
df_comparison
```

> ### **â”” LightGBM** 

```python
# â–¶ lightGBM
import lightgbm as lgbm
from lightgbm import LGBMClassifier

LGBM = lgbm.LGBMClassifier()
LGBM.fit(x_train, y_train)
```

```python
# â–¶ ì˜ˆì¸¡
y_pred_train = LGBM.predict(x_train)
y_pred_test = LGBM.predict(x_test)

print(classification_report(y_train, y_pred_train))
print(classification_report(y_test, y_pred_test))
```

```python
# â–¶ AUC
y_pred_train_proba = LGBM.predict_proba(x_train)[:, 1] 
y_pred_test_proba = LGBM.predict_proba(x_test)[:, 1] 

roc_score_train = roc_auc_score(y_train, y_pred_train_proba)
roc_score_test = roc_auc_score(y_test, y_pred_test_proba)

print("roc_score_train :", roc_score_train)
print("roc_score_test :", roc_score_test)
```
- ì–˜ë„ random forest ë³´ë‹¤ëŠ” ëœí•˜ì§€ë§Œ overfittingì¸ ë“¯

```python
lgbm_re = pd.DataFrame({ 'model' : ['LGBM'],
                      'f1_train' :  metrics.f1_score(y_train,y_pred_train),
                      'f1_test' : metrics.f1_score(y_test,y_pred_test),
                      'AUC_train' : roc_auc_score(y_train, y_pred_train_proba),
                      'AUC_test' : roc_auc_score(y_test, y_pred_test_proba),}
                    )

df_comparison = df_comparison.append(lgbm_re)
df_comparison
```

## **05. Model evaluation**
---
> **ì „ì²´ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€** 

* Hyper-parameter tunning ì „ ì „ì²´ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ

> **ê³¼ì í•©(Over-fitting)**

* train dataë¥¼ í™œìš©í•˜ì—¬ í•™ìŠµëœ ëª¨ë¸ì˜ ì„±ëŠ¥ê³¼ test dataë¥¼ ì˜ˆì¸¡í•œ ì„±ëŠ¥ì˜ Gapì´ í°ê²ƒ
* train dataì— ë„ˆë¬´ ì¹˜ì¤‘ë˜ì–´ í•™ìŠµí•œ ê²°ê³¼ë¡œ ë°œìƒí•¨
* ê³¼ì í•©ì´ ë°œìƒí•˜ë©´, Robustí•œ ëª¨ë¸ì´ ë  ìˆ˜ ì—†ê³ , ëª¨ë¸ ìš´ì˜ì‹œ ì¼ì •í•œ ì„±ëŠ¥ì„ ìœ ì§€í•  í™•ë¥ ì´ ë‚®ìŒ

> **ê³¼ì í•©(Over-fitting) í•´ê²°ë°©ë²•**

* ë‹¤ì–‘í•œ ë°©ë²•ë“¤ì´ ì¡´ì¬í•˜ì§€ë§Œ, í˜„ì—…ì—ì„œëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì ˆì„ í†µí•œ ê·œì œë¡œ ê³¼ì í•©ì„ ë°©ì§€
* Tree modleì„ ì˜ˆì‹œë¡œ max_depth(ë¶„ê¸°ì˜ ê¹Šì´) ê°’ì„ ì‘ì€ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ë¥¼ ëœ í•™ìŠµí•˜ê²Œ ë§Œë“¬
* ë” Robustí•œ ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•´ì„œ Train/Test dataë¥¼ í™œìš©í•˜ì—¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•˜ê³ , Validation setìœ¼ë¡œ ìµœì¢… ì„±ëŠ¥í‰ê°€ë¥¼ ì§„í–‰í•¨
* Train / Validation / Testì˜ ì„±ëŠ¥ ì°¨ì´ì˜ Gapì„ ìµœì†Œí™”í•˜ë©´ì„œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆëŠ” íŠœë‹ì„ ì§„í–‰ 

![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240114132547.png?raw=true)

```python
df_comparison = df_comparison.reset_index(drop=True)
df_comparison
```
