---
title: 2 5. Dimensionality Reduction
---

# Curse of Dimensionality: 차원의 저주
- 차원이 늘어남에 따라 같은 영역을 가지고 있음에도 전체 영역 대비 설명 가능한 데이터가 줄어들게 되는 현상을 말함
	- 1차원을 표현하기 위한 Data 수 : 1개
	- 2차원을 표현하기 위한 Data 수 : 3개
	- 3차원을 표현하기 위한 Data 수 : 7개
	- 4차원을 표현하기 위한 Data 수 : 15개 → 현실적으로 표현 불가능
	- P차원을 표현하기 위한 Data 수 : 𝟐^p-1
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240111133730.png?raw=true)
- 예) 숫자 손 글씨 Data : 16 x 16 x 3 dimension
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240111133837.png?raw=true)
- 지도학습을 통한 차원 축소 [2-1. Regression problem](https://code7ssage.github.io/code_file/2-1.-Regression-Problem//)의 feature selection 참조
	- Embedded : Regularized linear model, Random Forest using feature importance score
	- Filter : X’s와 Y의 Correlation, Chi-squared Test, Anova, Variance inflation Factor
	- Wrapper : Forward Selection, Backward Elimination, Stepwise Selection
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240111134307.png?raw=true)
- - 비지도학습을 통한 차원 축소
	- 고차원에서의 Variance, Distance 등을 저차원에서도 그 정보를 그대로 보존할 수 있도록 학습함
	- 비지도학습의 Best한 차원 축소는 고차원에서의 Data가 저차원에서의 Data와 똑같아야 함
		- Feature Extraction
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240111134352.png?raw=true)
	- [Light GBM](https://code7ssage.github.io/key_terms/Light-GBM//)의 GOSS, EFB와 비슷한 방식
- Selection vs Extraction
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240111134925.png?raw=true)
# PCA
- Principal Component Analysis
- PCA : 대표적인 Dimensionality Reduction에 쓰이는 기법 중 하나임 
- Label에 영향을 받지 않는 비지도 Feature Extraction 임 
- 지도학습에 맞게 Design 되지는 못함
- PCA는 “고차원의 Data 분산도를 저차원에서도 얼마 만큼 잘 보존해주는가“ 를 모티브로 한 알고리즘
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240111135106.png?raw=true)
- Covariance matrix : 시그마 기호로 표현하는 2차원 Data에서의 covariance matrix의는 아래와 같음
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240111135601.png?raw=true)
- x의 varianc는 y의 varianc보다 크므로 데이터는 **가로로 길쭉한 형태가 됨 
- y의 covariance가 양수이므로 **1, 3 사분면을 지나는 오른쪽 그림과 같은 형태가 됨**
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240111135710.png?raw=true)
- **eigenvector** (고유벡터) : 공분산 행렬에 의해 선형변환 되는 수많은 vector들 중, 변환되기 전과 변환된 후의 vector 방향이 똑같은 vector를 의미 함 
- 변환된 수 많은 Data 중 빨간색으로 표신 된 벡터 2개만은 변환 전과 후의 방향이 동일함
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240111135826.png?raw=true)
- 선형 변환이 되기 전과 후의 방향이 같은 벡터 v가 Eigenvector, 변화되는 길이의 λ 비율 값이  **Eigenvalue**
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240111135936.png?raw=true)
- Eigenvalue가 크면 클수록 고차원에서의 Data를 저차원에서 잘 표현한 것이라고 판단함 
- Eigenvalue를 내림 차순으로 정렬하고 가장 큰 N개 만큼 추출함 
- Plotting 하기 위해 차원 축소를 진행한 것이라면 2 ~ 3개만 추출
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240111140024.png?raw=true)
- 실습: [18. PCA Code](https://code7ssage.github.io/code_file/18.-PCA-Code//)
# T-SNE
- T-distributed Stochastic Neighbor Embedding
	- T-SNE는 높은 차원에서 비슷한 데이터 구조는 낮은 차원 공간에서 가깝게 대응하며, 비슷하지 않은 데이터 구조는 멀리 떨어져 있게 됨
	- 비선형적인 차원 축소 방법(특히 고차원의 Dataset을 시각화 하는 것에 굉장히 성능이 좋음)
	- 고차원 공간에서의 점들의 유사성과 그에 해당하는 저차원 공간에서의 점들의 **유사성을 계산**
	- 점들의 유사도는 A를 중심으로 한 정규 분포에서 확률 밀도에 비례하여 이웃을 선택하면 포인트 A가 포인트 B를 이웃으로 선택한다는 조건부 확률로 계산
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240111140242.png?raw=true)
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240111140327.png?raw=true)
- Kullback-Leibler divergence
	- 두 확률분포의 차이를 계산하는 데에 사용하는 함수로, 어떤 이상적인 분포에 대해, 그 분포를 근사하는 다른 분포를 사용해 샘플링을 한다면 발생할 수 있는 정보 엔트로피 차이를 계산한다.
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240111140415.png?raw=true)
- Olivetti faces datasets
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240111140451.png?raw=true)
- CalTech-101
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240111140513.png?raw=true)
- 실습: [19. T-SNE Code](https://code7ssage.github.io/code_file/19.-T-SNE-Code//)
# Autoencoder (Deep Learning)
- 입력이 들어왔을 때, 해당 입력 데이터를 최대한 Compression 시킨 후, Compressed data를 다시 본래의 입력 형태로 복원시키는 신경망 
- Data를 압축하는 부분을 **Encoder**라고 하며 복원하는 부분을 **Decoder**라고 부름 
- 압축 하는 과정에서 추출한 의미 있는 데이터 Z를 보통 Latent vector라고 부름
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240111140739.png?raw=true)
- Gradient Descent 활용 Weight 계속 update 시켜 줌
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240111140834.png?raw=true)
- Layer의 Depth가 깊어지면 깊어질 수록 Gradient Vanishing or Exploding 현상이 발생하게 됨 
- Gradient Vanishing or Exploding은 역전파(Backpropagation) 과정에서 마지막 Layer에서 멀어질수록 **Gradient 값이 매우 작아지거나 폭발하는 현상**을 말함
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240111140934.png?raw=true)
	- 해결: Dropout, Normalization 등등..
- 실습: [20. Autoencoder Code](https://code7ssage.github.io/code_file/20.-Autoencoder-Code//)