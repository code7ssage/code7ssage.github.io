---
title: 2 2. Classification problem
---

- [classification](https://code7ssage.github.io/classification//)참조
# Classification Loss Function은 옳고 그름 2가지 밖에 없음!
- **Decision Tree**
	- 가장 기본적인 Classification Model 
	- Decision Tree는 Regression Problem도 함께 적용 가능함
	- 데이터를 분석하여 이들 사이에 존재하는 패턴을 예측 가능한 **규칙(Rules)** 들의 조합으로 나타냄
	- **Rule Extraction** : 가장 중요하고 강력한 해석력을 가짐
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240105124748.png?raw=true)
- Classification Measuring Impurity for Split
	- 순도(Homogeneity)를 최대로 증가시키는 방향 
	- 불순도(Impurity) 혹은 불확실성(Uncertainty)을 최소로 감소시키는 방향
	- Measuring Impurity 1 : **Gini Index** (Max 0.5, 0일 때 가장 잘 나누어진 것)
		![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240105123246.png?raw=true)
		- 만약 파랑만 16개 있다면 I(A) = 1 -1^2=0, 이때 불순도는 0으로 최소이다.
	- Measuring Impurity 2 : **Entropy** (0일 때 가장 잘 나누어진 것)
		![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240105123512.png?raw=true)
	- Measuring Impurity 3 : Misclassification Error (잘 사용하지 않음)
- 기본적으로 Classification Model을 학습할 때 사용하는 **Binary Cross Entropy**
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240105123911.png?raw=true)
	- T=1일 때, P가 1에 가까울 수록 loss가 적다.
	- T=0일 때, P가 0에 가까울 수록 loss가 적다.
- **information gain**: split해서 얻어지는 불순도의 하락
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240105125149.png?raw=true)
	- 하지만 무수히 split하면 [Overfitting](https://code7ssage.github.io/Overfitting//)이 일어남 주의
- **Decision Tree Pruning**: 가지치기
	- 모든 Terminal node의 불순도가 0, 순도가 100%인 상태를 Full Tree라고 정의함 
	- Full tree를 생성한 뒤 적절한 수준에서 Terminal node를 잘라 줘야함
	- Max-depth: 2~5
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240105125354.png?raw=true)
# 평가는 어떻게?
- **TP(True Positive)**, 참양성: 예측한 값이 Positive이고 실제 값도 Positive인 경우 
- **FN(False Negative)**, 거짓음성: 예측한 값이 Negative이고 실제 값은 Positive인 경우 
- **FP(False Positive)**, 거짓양성: 예측한 값이 Positive이고 실제 Negative값은 인 경우 
- **TN(True Negative)**, 참음성: 예측한 값이 Negative이고 실제 값도 Negative인 경우
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240105125959.png?raw=true)
- **정분류율(Accuracy)** : 정확도는 직관적으로 모델 예측 성능을 나타내는 지표 
- **정밀도(Precision)**: 예측 Positive 중 실제도 Positive를 찾아낸 비율 
	(미처 잡아내지 못한 개수가 많더라도 더 정확한 예측이 필요한 경우) 
- **재현율(Recall)**: 실제 Positive중 올바르게 Positive를 예측해 낸 비율 
	(잘못 걸러내는 비율이 높더라도 참 값을 놓치는 일이 없도록, 의학, 불량) 
- **특이도(Specificity)**: 실제 Negative 중 올바르게 Negative를 찾아낸 비율
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240105130120.png?raw=true)
- accuracy 보다는 **F1 score**를 많이 씀
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240105130229.png?raw=true)

# Ensemble??
- 어떤 데이터를 학습할 때, **여러 개의 모델을 조화롭게 학습** 시켜 그 모델들의 예측 결과들을 이용하여 **더 정확한 예측 값**을 구할 수 있음
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240105132025.png?raw=true)
	- **"Ensmbles almost always work better"**: 한 개 보단 여러 모델 쓰는게 좋다!
- Ensemble(앙상블) 종류 
	- **Bagging** : Reduce the Variance 
		- **Stacking** : Use another prediction model 
	- **Boosting** : Reduce the Bias
- **Bagging**(Bootstrap Aggregating)
	- 다른 data set을 같은 algorithm을 적용
	- Bootstrap : 표본에서 추가적으로 표본을 **복원 추출**하고 각 표본에 대한 통계량을 다시 계산하는 것
	- N개의 Data가 있으면 N개를 Randomly하게 뽑아내서 새로운 Data Set을 구성함
	- Bootstrap을 진행하면 확률 상 뽑히지 못한 데이터는 36.8%가 됨
	- Aggregating: 
		Model Result Aggregating Method 1 : **Majority voting**(다수결)
		Model Result Aggregating Method 2 : Weighted voting 1
		Model Result Aggregating Method 3 : Weighted voting 2
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240105132442.png?raw=true)
- **Stacking**
	- 같은 data set, 다른 algorithms 사용
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240105132618.png?raw=true)
- **Boosting**
	- 못 맞춘 data set을 계속 다시 학습시키는 방법
	![image](https://github.com/code7ssage/code7ssage.github.io/blob/master/assets/attached%20file/Pasted%20image%2020240105132807.png?raw=true)
- 종류
	1. [Random Forest](https://code7ssage.github.io/Random-Forest/)
	2. [AdaBoost](https://code7ssage.github.io/AdaBoost//)
	3. [Gradient Boosting Machine](https://code7ssage.github.io/Gradient-Boosting-Machine//)
	4. [XG Boost](https://code7ssage.github.io/XG-Boost//)
	5. [Light GBM](https://code7ssage.github.io/Light-GBM//)